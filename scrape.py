import requests
from bs4 import BeautifulSoup

# the problem right now is that not all these pages have github links
# perhaps we could just take the ones we have, and since there are so many links it'll be good enough?
def scrape_website():
    for i in range(0, 60, 20):
        url = f"https://nvd.nist.gov/vuln/search/results?isCpeNameSearch=false&results_type=overview&form_type=Basic&search_type=all&startIndex={i}"
        response = requests.get(url)
        html_content = response.text
        soup = BeautifulSoup(html_content, 'html.parser')

        # find the words with CVE
        cve_tags = soup.find_all("a", href=lambda href: href and href.startswith('/vuln/detail'))
        for tag in cve_tags:
            scrape_cve(tag['href'])

def scrape_cve(cve_code):
    url = f"https://nvd.nist.gov{cve_code}"
    response = requests.get(url)
    html_content = response.text
    soup = BeautifulSoup(html_content, 'html.parser')

    # find the first a tag with an href attribute starting with 'github.com'
    github_link_tag = soup.find("a", href=lambda href: href and href.startswith('https://github.com'))

    # extract the href attribute if the tag is found
    if github_link_tag:
        github_link = github_link_tag['href']
        print(f"GitHub Link: {github_link}")
    else:
        print("GitHub link not found on the page.")

if __name__ == "__main__": 
    scrape_website()
    # scrape_cve("CVE-2024-1215")
